{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akhanna99/transformer/blob/main/transformer_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e62a6679",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e62a6679",
        "outputId": "ae391bfd-828a-4e94-bec9-0b65ed4c46b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Function of code block: Set up hyperparameters and other configurations for the model\n",
        "\n",
        "batch_size: Independent sequences being processed in parallel during training\n",
        "block_size: Max context length for predictions. input is divided into blocks of this size to create training samples\n",
        "n_embd = Embedding dimension\n",
        "n_head = Nnumber of attention heads in the multi-head self-attention mechanism\n",
        "n_layer = Number of transformer blocks in the model\n",
        "dropout = Used to regularise the model during training\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import collections\n",
        "import numpy as np\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 8 #32\n",
        "block_size = 512 #12 #16\n",
        "max_iters = 50000\n",
        "eval_interval = 50\n",
        "learning_rate = 1e-3 #3e-4 #6e-4\n",
        "'''\n",
        "weight_decay = 1e-1\n",
        "beta1 = 0.9\n",
        "betas2 = 0.95\n",
        "grad_clip = 1.0\n",
        "decay_lr =True\n",
        "10M parameters 300k tokens\n",
        "'''\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "eval_iters = 200\n",
        "n_embd = 768\n",
        "n_head = 8\n",
        "n_layer = 8\n",
        "dropout = 0.1\n",
        "# ------------\n",
        "initialTrain=True\n",
        "retrain=False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90a49fa0-67a4-4de4-bcc1-efc7ab4c7d70",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90a49fa0-67a4-4de4-bcc1-efc7ab4c7d70",
        "outputId": "b2ef3a51-babd-472f-a3f9-fff991784ff2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/MyDrive\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Function: Mount google drive to notebook so files can be accessed\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "import sys\n",
        "\n",
        "drive.mount('/content/MyDrive')\n",
        "\n",
        "path = '/content/MyDrive/MyDrive/Colab Notebooks/alice_in_wonderland.txt'\n",
        "\n",
        "with open(path, 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a9d35ea-bee5-4a8d-9dfd-319b3745ce2a",
      "metadata": {
        "id": "4a9d35ea-bee5-4a8d-9dfd-319b3745ce2a"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Function: Character-level tokenisation (not used)\n",
        "\"\"\"\n",
        "\n",
        "'''\n",
        "# here are all the unique characters that occur in this text\n",
        "# IMPORTANT:\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a702fbc-8964-4d00-870e-ff26717c8228",
      "metadata": {
        "id": "8a702fbc-8964-4d00-870e-ff26717c8228"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Function: Word-level tokenisation\n",
        "\"\"\"\n",
        "\n",
        "words = text.split()\n",
        "word_counts = {word: count for word, count in collections.Counter(words).items()}\n",
        "unique_words = sorted(word_counts.keys(), key=lambda word: word_counts[word], reverse=True)\n",
        "vocab_size = len(unique_words)\n",
        "stoi = {word: i for i, word in enumerate(unique_words)}\n",
        "itos = {i: word for i, word in enumerate(unique_words)}\n",
        "encode = lambda s: [stoi[word] for word in s.split()]\n",
        "decode = lambda l: ' '.join([itos[i] for i in l])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(encode(\"The Cheshire cat grinned\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7_IMwTWPTYg",
        "outputId": "57a5dc89-db07-4a0d-ace5-6eed121329a0"
      },
      "id": "u7_IMwTWPTYg",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[33, 589, 482, 1719]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode([33, 589, 482, 1719]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIRkgJ94PvnE",
        "outputId": "52eaa842-6241-49ca-b1cb-eb143aeb8a46"
      },
      "id": "hIRkgJ94PvnE",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Cheshire cat grinned\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82cb405e-2336-4696-9321-af249f4c6e2e",
      "metadata": {
        "id": "82cb405e-2336-4696-9321-af249f4c6e2e"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Function: Data is split into train (90%) and test (10%) sets\n",
        "\"\"\"\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac1ee2e7-bdc6-40de-8e0c-2fd0eb64a909",
      "metadata": {
        "id": "ac1ee2e7-bdc6-40de-8e0c-2fd0eb64a909"
      },
      "outputs": [],
      "source": [
        "def get_batch(split):\n",
        "    \"\"\"\n",
        "    Function: Generate small batches of data\n",
        "    ix: random indices between the range of the length of the data and the block size\n",
        "    Output: inputs (x) and targets (y)\n",
        "    \"\"\"\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    \"\"\"\n",
        "    Function: Estimates the loss of the model on both the training and validation datasets\n",
        "    model.eval(): Puts the model in evaluation mode. This will cause certain configurations\n",
        "    such as dropout and batch normalisation to behave differently than during training. This\n",
        "    ensures the model produces consistent results during evaluation.\n",
        "    Output: 'out' - Mean loss values for both the train and val datasets\n",
        "    \"\"\"\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" Function: This is a representation of one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Function: Performs the self-attention computation\n",
        "        B - batch size, T - sequence length, C - vocab size\n",
        "        \"\"\"\n",
        "\n",
        "        # 1. Linear projections applied to x to obtain key, query and value vectors\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "\n",
        "        # 2. Compute attention scores (\"affinities\") using matrix multiplication\n",
        "        # scale by C**-0.5 to normalise attention scores\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "\n",
        "        ## 3. Mask using `tril` to ensure attention only applied to current and previous tokens\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "\n",
        "        # 4. Normaise attention scores using softmax function\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "\n",
        "        # 5. Apply dropoiut to attention scores to prevent overfitting during training\n",
        "        wei = self.dropout(wei)\n",
        "\n",
        "        # 6. Perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "\n",
        "        # 7. 'out' represents the contexualised attention matrix of the attention head\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" Function: To run multiple heads of self-attention running in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Function: Combine the attention outputs from all the attention heads into one matrix\n",
        "        \"\"\"\n",
        "        # 1. Concatenate output of all attention head\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "\n",
        "        # 2. linear projection transforms the concatenated matrix back to original embedding dimention\n",
        "        # 3. dropout applied to prevent over-fitting\n",
        "        out = self.dropout(self.proj(out))\n",
        "\n",
        "        # 4. projection back into the residual pathway\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\"Function: Simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            # linear transformation to create the 'hidden' layer\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            # ReLU activation function introduces non-linearity to the neural network\n",
        "            nn.ReLU(),\n",
        "            # projection layer going back into the residual pathway\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Function: A transformer block that does communication followed by computation.\n",
        "    Allows infromation to be communicated and processed in parallel through multiple\n",
        "    attention heads and then combined using residual connections \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 'fork off' and do some communication and then come back\n",
        "\n",
        "        # 1. Apply self-attention mechanism to x and add this to the original input tensor x\n",
        "        # allows information to flow through Transformer block without getting lost in the layers\n",
        "        # through residual connections\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "\n",
        "        # 2. Updated matrix x is passed through feedforward neural network\n",
        "        # result is added back to tensor x through residual connections\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "\n",
        "        # 3. Final output represents the processed tensor after both self-attention\n",
        "        # and feedforward computation\n",
        "        return x\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "    \"\"\" Large language model that generates sequences of tokens based on a given input text\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # converts each token to coressponding token embeddings\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        # creates another embedding layer to add positional encodings\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        # stacks multiple transformer blocks / layers\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        # final layer normaisation and linear layer to map output embeddings to logits over vocab size\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        \"\"\"\n",
        "        Function: Computes the model's predictions (logits) for each token in vocab at each\n",
        "        position in the generated sequence\n",
        "        Training: Calculates logits and loss based on targets provided\n",
        "        Testing: Calculates logits but loss is not calculated\n",
        "        \"\"\"\n",
        "        B, T = idx.shape # idx and targets are both (B,T) tensor of integers\n",
        "\n",
        "        # 1. Token embedding look-up\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "\n",
        "        # 2. Positional encoding look-up\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "\n",
        "        # 3. Token embedding and positional encodings are combined to form input to transformer blocks\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "\n",
        "        # 4. Inputs passed through stack of transformer blocks\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "\n",
        "        # 5. Output of transformer blocks normalised\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "\n",
        "        # 6. Embeddings converted to logits over the vocab size\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        # 7. If in train mode, loss is calculated using cross-entropy loss between predicted logits and targets,\n",
        "        # otherwise, loss is set to None\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        \"\"\"\n",
        "        Function: Generates new tokens iteratively, taking context (idx) into account\n",
        "        idx: (B, T) array of indices in the current context\n",
        "        max_new_tokens: The number of new tokens to be iteratively generated\n",
        "        \"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            # 1. Take the last `block_size` tokens from the curreent context `idx_cond`\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "\n",
        "            # 2. Use the model to predict the next token probabilities based on current context\n",
        "            logits, loss = self(idx_cond)\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "\n",
        "            # 3. Sample from the distribution to find the index of the next token\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "\n",
        "            # 4. Append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "\n",
        "        # 5. Output the original context and the generated tokens\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62b6f8a1-534d-40b5-8558-0b52bfa21a66",
      "metadata": {
        "id": "62b6f8a1-534d-40b5-8558-0b52bfa21a66"
      },
      "outputs": [],
      "source": [
        "class EarlyStopper:\n",
        "    \"\"\"Function: Implements early stopping during model training.\n",
        "    Keeps track of the min validation loss and no. consecutive epochs were the val loss hasn't improved\n",
        "    If the val loss doesn't improve for a specifed no. epochs it will return True to early stop\"\"\"\n",
        "\n",
        "    def __init__(self,patience=1,min_delta=0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter=0\n",
        "        self.min_val_loss = np.inf\n",
        "\n",
        "    def early_stop(self, val_loss, model):\n",
        "        if val_loss < self.min_val_loss:\n",
        "            # if val loss has improved, update the min val loss and reset the counter\n",
        "            self.min_val_loss = val_loss\n",
        "            self.counter = 0\n",
        "            # save the trained model with the best val loss\n",
        "            model_path = \"trained_model.pth\"\n",
        "            torch.save(model.state_dict(), model_path)\n",
        "\n",
        "        # if val loss has not improved, increment the counter\n",
        "        elif val_loss > (self.min_val_loss + self.min_delta):\n",
        "            self.counter += 1\n",
        "            # if the counter exceeds patience, return True for early stopping\n",
        "            if self.counter >= self.patience:\n",
        "                return True\n",
        "            return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7f0fa7b-17f3-4136-8c6a-3e13ea973637",
      "metadata": {
        "id": "d7f0fa7b-17f3-4136-8c6a-3e13ea973637"
      },
      "outputs": [],
      "source": [
        "def train_model(model, optimizer, max_iters, eval_interval):\n",
        "    \"\"\"Function: Trains the transformer using an optimiser\"\"\"\n",
        "\n",
        "    early_stopper = EarlyStopper(patience = 5, min_delta = 0.01)\n",
        "    for iter in range(max_iters):\n",
        "        if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "            losses = estimate_loss()\n",
        "            print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "        if early_stopper.early_stop(losses['train'],model):\n",
        "            print('Early stopping')\n",
        "            break\n",
        "\n",
        "        # 1. Sample a batch of data (inputs and targets)\n",
        "        xb, yb = get_batch('train')\n",
        "\n",
        "        # 2. Set the model to 'train' model and evaluate the logits and loss\n",
        "        model.train()\n",
        "        logits, loss = model(xb, yb)\n",
        "\n",
        "        # 3. Zero the gradients of the optimiser to clear accumulated gradients for model parameters\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        # 4. Perform backpropogation to compute gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # 5. Update model parameters\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e1a0df8-fa27-4665-85cb-f2d64c6c8291",
      "metadata": {
        "id": "4e1a0df8-fa27-4665-85cb-f2d64c6c8291"
      },
      "outputs": [],
      "source": [
        "\"\"\"Initial training of the transformer, must be run once once to set-up initial \"trained_model.pth\" \"\"\"\n",
        "\n",
        "if initialTrain:\n",
        "    model = BigramLanguageModel()\n",
        "    model = model.to(device)\n",
        "\n",
        "    # set to training mode\n",
        "    model.train()\n",
        "\n",
        "    # optimiser to update the model parameters during training\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "    train_model(model, optimizer, max_iters, eval_interval)\n",
        "\n",
        "    # save the trained model\n",
        "    model_path = \"trained_model.pth\"\n",
        "    torch.save(model.state_dict(), model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09015d9a-a326-4a71-bca8-9825bc43b233",
      "metadata": {
        "id": "09015d9a-a326-4a71-bca8-9825bc43b233"
      },
      "outputs": [],
      "source": [
        "\"\"\"Loads the currently saved transformer model and re-trains it\"\"\"\n",
        "\n",
        "if retrain:\n",
        "    model = BigramLanguageModel()\n",
        "\n",
        "    # load the latest saved model\n",
        "    model_path = \"trained_model.pth\"\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "    # set to training mode and train\n",
        "    model = model.to(device)\n",
        "    model.train()\n",
        "    learning_rate = 1e-2\n",
        "    max_iters = 1500\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "    train_model(model, optimizer, max_iters, eval_interval)\n",
        "\n",
        "    # save the trained model\n",
        "    model_path = \"trained_model.pth\"\n",
        "    torch.save(model.state_dict(), model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bbcd3c5-a0f7-4837-8b46-a4e079284f25",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bbcd3c5-a0f7-4837-8b46-a4e079284f25",
        "outputId": "51a2690a-8d4d-4e6a-b72a-ba939d7b775b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65.234618 M parameters\n",
            "Alice sat on the rabbit with tears into it: there were a Duck and a Dodo, a Lory and an Eaglet, and several other curious creatures. Alice led the way, and the whole party swam to the shore. CHAPTER III A Caucus-Race and a Long Tale They were indeed a queer-looking party that assembled on the bank--the birds with draggled feathers, the animals with their fur clinging close to them, and all dripping wet, cross, and uncomfortable. The first question of course was, how to get dry again: they had a consultation about this, and after a few minutes it seemed quite natural to Alice to find herself talking familiarly with them, as if she had known them all her life. Indeed, she had quite a long argument with the Lory, who at last turned sulky, and would only say, `I am older than you, and must know better'; and this Alice would not allow without knowing how old it was, and, as the Lory positively refused to tell its age, there was no more to be said. At last the Mouse, who seemed to be a person of authority among them, called out, `Sit down, all of you, and listen to me! I'LL soon make you dry enough!' They all sat down at once, in a large ring, with the Mouse in the middle. nothing.' `Nobody seems to dry enough!' They all the middle. Alice kept her eyes anxiously fixed on it, for she felt sure she would catch a bad cold if she did not get her face very soon. `Ahem!' said the Mouse with an important air, `are you all ready? This is the driest thing I know. Silence all round, if you please! \"William the Conqueror, whose cause was favoured by the pope, was soon submitted to by the English, who\n"
          ]
        }
      ],
      "source": [
        "\"\"\" Inference code for generating new tokens \"\"\"\n",
        "\n",
        "# Load the latest saved trained model\n",
        "model = BigramLanguageModel()\n",
        "model_path = \"/content/MyDrive/MyDrive/Colab Notebooks/trained_model_joe.pth\"\n",
        "model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
        "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n",
        "model = model.to(device)\n",
        "\n",
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Set up a random index as the starting context for the text generation\n",
        "# context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "\n",
        "# Specify an initial prompt for the text generation\n",
        "prompt = \"Alice sat on the rabbit\"\n",
        "context2 = torch.tensor(encode(prompt), dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "# Generate and print the new tokens\n",
        "generated_tokens = model.generate(context2, max_new_tokens=300)[0].tolist()\n",
        "generated_text = decode(generated_tokens)\n",
        "print(generated_text)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}